\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=blue,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}
\usepackage{adjustbox}

\input{../Comments}
\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Date October 27 & 1.0 & Adding NFR testing plan\\
Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

~\\
\wss{The intention of the VnV plan is to increase confidence in the software.
However, this does not mean listing every verification and validation technique
that has ever been devised.  The VnV plan should also be a \textbf{feasible}
plan. Execution of the plan should be possible with the time and team available.
If the full plan cannot be completed during the time available, it can either be
modified to ``fake it'', or a better solution is to add a section describing
what work has been completed and what work is still planned for the future.}

\wss{The VnV plan is typically started after the requirements stage, but before
the design stage.  This means that the sections related to unit testing cannot
initially be completed.  The sections will be filled in after the design stage
is complete.  the final version of the VnV plan should have all sections filled
in.}

\newpage

\tableofcontents

\listoftables
\wss{Remove this section if it isn't needed}

\listoffigures
\wss{Remove this section if it isn't needed}

\newpage

\section{Symbols, Abbreviations, and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations, or acronyms --- you can simply reference the SRS
  \citep{SRS} tables, if appropriate}

\wss{Remove this section if it isn't needed}

\newpage

\pagenumbering{arabic}

This document ... \wss{provide an introductory blurb and roadmap of the
  Verification and Validation plan}

\section{General Information}

\subsection{Summary}
The software system being tested is \textbf{Sanskrit Manuscript Reconstruction Platform}, a web-based research tool designed to assist scholars in reconstructing fragmented Buddhist manuscripts through computational analysis. The system integrates machine learning, computer vision, and database management to enable users to upload digitized manuscript fragments, perform automated preprocessing and orientation correction, and generate similarity based match suggestions across fragments. 
\newline
The platform consists of a front-end interface for visualization and user interaction, a back-end API for data management and orchestration, and multiple AI/ML components responsible for text extraction, edge and damage pattern analysis, and handwriting classification.  
\newline
The verification and validation process will ensure that all core functionalities, including fragment upload, orientation correction, transcription assistance, and match discovery, perform correctly, reliably, and consistently across diverse manuscript inputs. It will also verify that data integrity, user authentication, and safety-critical requirements identified in the Hazard Analysis are met. 
\newline
Testing will be conducted incrementally across all major milestones (Proof of Concept, Revision 0, and Revision 1) using a combination of unit testing, integration testing, usability testing, and system-level validation to confirm that the software meets its specified functional and non-functional requirements as outlined in the SRS.
  
\subsection{Objectives}

The primary objective of this Verification and Validation (V\&V) Plan is to ensure that the \textbf{Sanskrit Manuscript Reconstruction Platform} meets its defined functional and non-functional requirements, operates reliably under typical research conditions, and achieves sufficient usability for its intended audience of Buddhist Studies scholars. The V\&V process aims to build confidence in the correctness, stability, and scholarly utility of the system.

\textbf{In-Scope Objectives:}
\begin{itemize}
    \item \textbf{Functional correctness:} Verify that all major workflows (fragment upload, image normalization, fragment matching, OCR transcription, and session management) produce accurate and expected outcomes.
    \item \textbf{Reliability:} Validate that the system performs consistently across multiple sessions and input types, with appropriate handling of errors, interruptions, and data corruption.
    \item \textbf{Data integrity and security:} Ensure that user data, manuscript fragments, and annotations are stored and transmitted securely, as outlined in the Safety and Security Requirements.
    \item \textbf{Usability and accessibility:} Evaluate that scholars can intuitively use the system to complete core reconstruction tasks with minimal training, supported by the usability study and user manual deliverables.
    \item \textbf{Model transparency:} Confirm that AI-generated suggestions (match rankings, OCR outputs, classifications) include interpretable confidence levels and require explicit human confirmation before acceptance.
    \item \textbf{Traceability:} Establish end-to-end verification between requirements, test cases, and hazard mitigations to ensure all critical safety-related functionalities are validated.
\end{itemize}

\textbf{Out-of-Scope Objectives:}
\begin{itemize}
    \item \textbf{External library verification:} The correctness of third-party libraries and frameworks (e.g., OpenCV, PyTorch, TensorFlow) will be assumed based on their established reliability and community validation.
    \item \textbf{Extensive performance benchmarking:} While basic response-time and load-handling tests will be conducted, large-scale benchmarking under production conditions is beyond the project’s resource and time constraints.
    \item \textbf{Cross-platform hardware testing:} The V\&V plan focuses on web deployment via modern browsers and does not include testing on specialized hardware or legacy systems.
    \item \textbf{Formal verification:} Mathematical proofs or model checking of algorithms are not feasible within the capstone timeline and are deferred to potential future work.
\end{itemize}

The objectives and their limitations reflect the practical constraints imposed on the team (mainly time and resources) while ensuring comprehensive coverage of functionality, usability, and safety-critical elements central to the platform’s success.


\subsection{Challenge Level and Extras}

This year’s capstone projects do not include a challenge level. Evaluation will focus on system completeness, correctness, usability, and overall technical quality.

\textbf{Extras:}

Two approved extras are included as part of this project to enhance the evaluation of usability and maintainability.

\begin{itemize}
    \item \textbf{User Manual:}  
    A comprehensive user manual will be developed to assist scholars in using the platform effectively. It will describe the main workflows such as fragment upload, preprocessing, matching, and transcription, and provide troubleshooting guidance. The manual will also include annotated screenshots and short tutorials covering both basic and advanced usage scenarios.

    \item \textbf{Usability Report:}  
    A formal usability evaluation will be conducted to assess the system’s ease of use, learnability, and effectiveness for non-technical users. Feedback will be collected from scholars in Buddhist Studies and related fields through observation and structured task completion. The resulting report will summarize key usability metrics, identify design pain points, and provide recommendations for improvement.
\end{itemize}

These extras directly support the project’s primary goals of accessibility and scholarly usability by ensuring that end users can efficiently interact with complex AI-driven tools through a clear, guided interface.

\subsection{Relevant Documentation}

This Verification and Validation Plan references all major project documents that define the system’s motivation, requirements, development strategy, safety considerations, and user-facing deliverables for the \textbf{Sanskrit Manuscript Reconstruction Platform}. These documents collectively establish the foundation for testing and traceability throughout the project lifecycle.

\begin{itemize}
    \item \textbf{Problem Statement and Goals Document} (\citet{ProblemStatement})  
    Defines the project motivation, objectives, and expected outcomes for reconstructing fragmented Buddhist manuscripts.

    \item \textbf{Development Plan} (\citet{DevelopmentPlan})  
    Outlines the workflow structure, team responsibilities, scheduling, coding standards, and continuous integration process.

    \item \textbf{Software Requirements Specification (SRS)} (\citet{SRS-Sanskrit-Ciphers})  
    Specifies all functional and non-functional requirements, including detailed system components, interfaces, and prioritized behaviors.

    \item \textbf{Hazard Analysis Document} (\citet{HazardAnalysis})  
    Identifies potential hazards, risk factors, and corresponding safety and security requirements that inform this V\&V plan.

    \item \textbf{Design Documents (MG and MIS)} (\citet{DesignDocs})  
    To be developed in later phases. These will provide architectural decomposition, module definitions, and design rationale.

    \item \textbf{User Manual} (\citet{UserManual})  
    Provides detailed guidance for scholars on using the platform’s core features, including uploading, preprocessing, matching, and transcription.

    \item \textbf{Usability Report} (\citet{UsabilityReport})  
    Summarizes results from usability testing and participant feedback to evaluate ease of use, learnability, and overall effectiveness.
\end{itemize}

Together, these documents establish a complete record of the project’s development and verification process, ensuring traceability between requirements, design, testing, and evaluation.

\section{Plan}

This section presents the overall strategy for Verification and Validation (V\&V) of the project. It outlines how we will organize people and activities, how key artifacts will be reviewed and tested, and how results will inform subsequent work. The goal is to provide a clear, practical roadmap for building confidence in the system from requirements through delivery.


\subsection{Verification and Validation Team}

This subsection defines the project roles responsible for planning, executing, and reviewing V\&V activities. Each role has clear
primary responsibilities and an assigned independent Reviewer to ensure separation of duties and objective appraisal.
Roles may rotate by phase (requirements, design, implementation) to match expertise and workload, but accountability remains
with the named owner for the current milestone. The table below summarizes the assignments.

\begin{table}[h]
\centering
\caption{V\&V Roles and Responsibilities}
\begin{adjustbox}{max width=\textwidth,center}
\begin{tabular}{p{3.2cm} p{7.2cm} p{3cm} p{2cm}}
\toprule
\textbf{Role} & \textbf{Primary Responsibilities} & \textbf{Person} & \textbf{Reviewer}\\
\midrule
V\&V Lead & Manages the development and execution of the V\&V plan, oversees test protocols, ensures compliance, and leads root cause analysis for issues found during testing  & Dylan Garner & Aswin Kuganesan\\
V\&V Manager & Defines the overall V\&V strategy, coordinates activities across the project, manages schedules and resources, and forecasts workloads & Yousef Shahin & Umar Khan \\
Test Architect & Builds Req$\leftrightarrow$Test traceability; drafts system and NFR tests & Omar El-Aref & Yousef Shahin \\
V\&V Engineer &  Develops and executes test plans and procedures, designs test cases, documents results, and troubleshoots issues.  & Umar Khan & Dylan Garner \\
Quality Assurance & Often works closely with the V\&V team to ensure the product meets quality standards throughout the development lifecycle & Aswin Kuganesan & Omar El-Aref\\
Supervisor &  External reviewer; approves SRS baselining and risk mitigations & Dr. Shayne &  \\
\bottomrule
\end{tabular}
\end{adjustbox}
\end{table}

\subsection{SRS Verification}

This subsection defines how we will verify that the SRS is correct, complete, consistent, measurable, and feasible. We use three complementary approaches: guided reviews by TAs/peers, a supervisor-facing survey and interview that validate fundamentals in non-technical terms (translated into technical requirements), and continuous conformance checks during design/implementation against SRS use cases and fit criteria. Each approach includes instruments, process, and success metrics.

\subsubsection*{TA/Peer Guided Review (Structured Feedback)}
\textbf{Instrument.} A checklist-driven review by peers and TAs to ensure we are staying in line with the fundemntals of our requirements. Issues are filed in GitHub as feedback from TAs and peers as well as feedback on Avenue which is more detailed from the TA\\
\textbf{Process.}
\begin{enumerate}
  \item After handing in the SRS document our peers go through the document and highlight areas to improve on.
  \item Once our peers have given us feedback we are to address the feedback
  \item The TA will also go through the SRS document and provide more in depth feedback to help with the document overall
  \item Once we get the feedback for the TA we create an Issue in GitHub and address the feedback given to us
\end{enumerate}
\textbf{Success metrics.}
\begin{itemize}
  \item \textbf{Measurability coverage:} $\geq 100\%$ of feedback has been addressed
  \item \textbf{Ambiguity count:} \(\leq 2\) open ambiguity items after rework.
  \item \textbf{Defect turn-around time (TAT):} All feedback has been addressed within 72 hours of being given the feedback (with a grace period of 48 hours for the entire team)
  \item \textbf{Link/xref errors:} $0$ build warnings for missing references.
\end{itemize}

\subsubsection*{Supervisor Validation via Survey plus Interview (Non-technical \textrightarrow{} Technical)}
\textbf{Instrument.} A 10-question survey focused on \emph{fundamental product expectations}, followed by a 30\,min interview. Example survey items:
\begin{enumerate}
  \item ``The SRS describes how fragments will be \emph{prepared} (orientation/cleaning) before analysis.'' (1--5)
  \item ``The SRS explains how \emph{candidate matches} will be presented for scholarly judgment.'' (1--5)
  \item ``The SRS addresses \emph{trust signals} (e.g., confidence scores, overlays) for suggested matches.'' (1--5)
  \item ``Privacy and access for \emph{unpublished images} are clearly described.'' (1--5)
  \item Open: ``What essential research task is missing or under-specified?''
\end{enumerate}
\textbf{Process.}
\begin{enumerate}
  \item Send SRS and survey 48 hours in advance; collect responses.
  \item Hold interview; capture clarifications and priorities (top~3 must-haves).
  \item \emph{Translation step:} Requirements Analyst converts each non-technical concern into one or more technical requirements (G/S with Fit Criteria), files issues, updates traceability.
\end{enumerate}
\textbf{Success metrics.}
\begin{itemize}
  \item \textbf{Coverage score:} average score $\geq 4.0$ on survey.
  \item \textbf{Gap closure:} 100\% of supervisor ``missing/under-specified'' items mapped to new/updated requirements within 1~week.
  \item \textbf{Traceability updates:} new items appear in Req$\rightarrow$Test matrix with an assigned test type and owner.
\end{itemize}

\subsubsection*{Ongoing Conformance During Build (Design/Code \textrightarrow{} SRS)}
\textbf{Instrument.} A checklist that is shared with the team and has each module mapped to a requirement. \\
\textbf{Process.}
\begin{enumerate}
  \item For each S.4 scenario in the SRS, create at least one system test; link the implementing issue/Pull Request (PR).
  \item On each PR, the author selects related requirement from the SRS document.
  \item Monthly conformance audit: Each month the team will highlight what percentage of the requirements is done or being completed.
  \item The team will also hihglight each month if they encountered a requirement that the SRS missed and make sure it is added within 48 hours of finding the missing requirement.
  \item If code conflicts with SRS, open change request: either update SRS (with rationale) or refactor code.
\end{enumerate}
\textbf{Success metrics.}
\begin{itemize}
  \item \textbf{Traceability coverage:} $\geq 95\%$ of modules or requirments can be linked to code module.
  \item \textbf{Use-case alignment:} 100\% of G.5 use cases have a corresponding S.4 detailed scenario and at least one test case.
\end{itemize}


\subsection{Design Verification}

This subsection defines how we will verify that the \emph{design} (architecture, detailed design, and model design) is
\textbf{sound, implementable, and consistent} with the SRS. We use three complementary techniques with objective
success criteria.

\subsection*{DV-1: Unified Modeling Language (UML) Design \texorpdfstring{$\leftrightarrow$}{↔} Code Modules}

\textbf{Approach.} Maintain a living UML class diagram for core modules (front-end, backend services, Machine Learning (ML) components).
Verify code conformance on each milestone via static review (e.g., presence of classes/interfaces,
key method signatures, dependency directions).
\begin{itemize}
  \item \textbf{Procedure:}
  \begin{enumerate}
    \item Identify \emph{must-exist} types and interfaces in the UML for the current milestone.
    \item Map each UML element to its code module (file/path, namespace, class/interface name).
    \item Keep a checklist of the modules that have been mapped and the ones that still need too get created.
    \item As the design starts getting implemented we can start adjusting the UML diagram as needed.
  \end{enumerate}
  \item \textbf{Success metrics:}
  \begin{itemize}
    \item \emph{Coverage:} $\geq 95\%$ of \emph{must-exist} UML elements mapped to code modules.
    \item \emph{Directional integrity:} $0$ violations of designated dependency rules.
  \end{itemize}
\end{itemize}

\subsection*{DV-2: Structured Walkthroughs (Code/Design)}
We will use lightweight, structured walkthroughs for code and design implementations
\paragraph{Author}
\begin{itemize}
  \item \textbf{What they do:} Presents the change (PR), states intent, scope, and known risks; answers questions.
  \item \textbf{How it works:} In a meeting they will share their screen and present the changes they made as well as the logic behind their changes
  \item \textbf{Why it rotates:} The author is simply whoever made the change for that task; authorship necessarily varies by feature or document section.
\end{itemize}

\paragraph{Reviewer}
\begin{itemize}
  \item \textbf{What they do:} Performs the detailed critique using a checklist (correctness, testability, traceability, compliance).
  \item \textbf{How it works:} Reviews asynchronously before the session; in the session, walks through findings; files issues or suggests concrete edits.
  \item \textbf{Why it rotates:} Review expertise should match the change (e.g., ML for matching model code, accessibility for User Interface (UI) text), so the most relevant teammate reviews.
\end{itemize}

\paragraph{Walkthrough Lead}
\begin{itemize}
  \item \textbf{What they do:} Facilitates the session, keeps time, ensures the checklist is followed, resolves scope creep, and states clear outcomes.
  \item \textbf{How it works:} Opens with goals and exit criteria; keeps discussion to evidence; confirms owner and due date for each action.
  \item \textbf{Why it rotates:} Lead alternates between V\&V Lead and Test Architect depending on topic (documents vs.\ tests/code), ensuring domain-appropriate facilitation.
\end{itemize}

\paragraph{Audience / Notetaker}
\begin{itemize}
  \item \textbf{What they do:} Attends for situational awareness; the designated notetaker records decisions, action items, and deferments.
  \item \textbf{How it works:} Notes are captured and action items become GitHub issues (\texttt{blocker/major/minor}) with assignees and deadlines.
  \item \textbf{Why it rotates:} Anyone can take notes; rotating spreads context and avoids overloading a single person.
\end{itemize}

\vspace{0.5em}
\noindent\textbf{Process (applies to both code and document walkthroughs).}
\begin{enumerate}
  \item \textbf{Prep (T\,$-$\,24h):} Author posts link(s) to PR , a 1--2 paragraph summary, and relevant checklists.
  \item \textbf{Asynchronous review:} Reviewer annotates items and runs applicable checks.
  \item \textbf{Session (30 minutes):} Lead states goals \& exit criteria; Author presents; Reviewer walks through findings; decisions and actions are recorded.
  \item \textbf{Exit criteria:} All \emph{blocker/major} items assigned with due dates; PR updated or issues filed; next steps acknowledged by owners.
  \item \textbf{Follow-up (72h):} Author closes items; Reviewer verifies fixes; Lead updates the review log.
\end{enumerate}

\vspace{0.25em}
\noindent\textbf{Why roles change per task.}
Changes originate in different parts of the system (UI, backend, ML). The \emph{author} is the contributor for that change; the \emph{reviewer} is selected for the best subject-matter fit; the \emph{lead} alternates to match topic; the \emph{notetaker} rotates to share context. This rotation ensures expertise, fairness in workload, and higher defect detection while remaining lightweight and feasible.


\subsection*{DV-3: ML Design Soundness (Regularization/Robustness)}
\textbf{Approach.} Verify that AI/ML components are designed to generalize: regularization and robustness are evaluated with
stress tests.
\begin{itemize}
  \item \textbf{Controls implemented:} dropout (or equivalent), data augmentation, early stopping, and (where applicable) ensembles.
  \item \textbf{Ablation plan:} compare baseline vs.\ dropout, augmentation, ensemble; report mean$\pm$sd over $k$-fold Cross Validation.
  \item \textbf{Robustness:} evaluate on mildly perturbed inputs (rotation, illumination, small occlusion); report metric deltas.
  \item \textbf{Success metrics:}
    \begin{itemize}
      \item Will mainly be measured by if we found a better more robust model. Regardless of the outcome it will be a success since we have tested the models ability to generalize.
    \end{itemize}
  \item \textbf{Design outcome:} if criteria fail, document change in an ADR (e.g., add augmentation X, adjust dropout $p$, switch loss) and retest.
\end{itemize}

\subsection{Verification and Validation Plan Verification}

The V\&V plan is itself an artifact that must be verified and improved over time. We will use
peer/TA reviews, a structured test-plan walkthrough \& retrospective focused on functional and nonfunctional tests

\subsection*{PV-1: Classmate and TA Reviews}
\textbf{What:} External review by classmates and a course TA to assess clarity, completeness, and feasibility. \\
\textbf{How:}
\begin{itemize}
  \item Have a completed version of the V\&V plan submitted.
  \item Class peers review the plan and give feedback in an Issue on the GitHub.
  \item TA reviews the plan and confirms scope, grading-aligned expectations, and missing evidence.
  \item Each team memeber will create an Issue on the GitHub and fix the comments they were given on their respective parts.
\end{itemize}
\textbf{Success:} $\geq 95\%$ of actions from peer/TA reviews closed within 7 days.

\subsection*{PV-2: Test-Plan Walkthrough \& Retrospective (Functional \& Nonfunctional)}
\textbf{What:} A structured meeting to walk through current system tests (functional) and NFR tests (accessibility, privacy, performance, usability), emphasizing what has worked, what has not, and how to adapt. This will be quite similar to the walkthrough mentioned ion susection 3.1. \\
\textbf{Inputs:} Current test catalog, pass/fail history, known pain points, and a walkthrough checklist covering: specificity of steps/data, pass/fail criteria, traceability to SRS Req IDs, reusability, and stakeholder relevance. \\
\textbf{How:}
\begin{enumerate}
  \item Lead presents the V\&V Plan document and walks through the sections asking if the team has any doubts with the currecnt section.
  \item If a team member does have doubts about a section they would then specify. (If not the Lead moves on to the next section).
  \item The team comes up with a solution to address the doubt and clear up any misunderstadning.
  \item Team records adjustments in an Issue on the GitHub with owner and due date.
\end{enumerate}
\textbf{Success:}
\begin{itemize}
  \item Executability: 100\% of sampled tests can be executed by a third party without clarification after fixes.
  \item Specificity: Each test specifies input data, exact steps, expected outputs, and pass thresholds.
  \item Adaptation throughput: $\geq 95\%$ of improvement actions closed by next walkthrough.
\end{itemize}

\subsection{Implementation Verification}

\wss{You should at least point to the tests listed in this document and the unit
  testing plan.}

\wss{In this section you would also give any details of any plans for static
  verification of the implementation.  Potential techniques include code
  walkthroughs, code inspection, static analyzers, etc.}

\wss{The final class presentation in CAS 741 could be used as a code
walkthrough.  There is also a possibility of using the final presentation (in
CAS741) for a partial usability survey.}

\subsection{Automated Testing and Verification Tools}

\wss{What tools are you using for automated testing.  Likely a unit testing
  framework and maybe a profiling tool, like ValGrind.  Other possible tools
  include a static analyzer, make, continuous integration tools, test coverage
  tools, etc.  Explain your plans for summarizing code coverage metrics.
  Linters are another important class of tools.  For the programming language
  you select, you should look at the available linters.  There may also be tools
  that verify that coding standards have been respected, like flake9 for
  Python.}

\wss{If you have already done this in the development plan, you can point to
that document.}

\wss{The details of this section will likely evolve as you get closer to the
  implementation.}

\subsection{Software Validation}

\wss{If there is any external data that can be used for validation, you should
  point to it here.  If there are no plans for validation, you should state that
  here.}

\wss{You might want to use review sessions with the stakeholder to check that
the requirements document captures the right requirements.  Maybe task based
inspection?}

\wss{For those capstone teams with an external supervisor, the Rev 0 demo should 
be used as an opportunity to validate the requirements.  You should plan on 
demonstrating your project to your supervisor shortly after the scheduled Rev 0 demo.  
The feedback from your supervisor will be very useful for improving your project.}

\wss{For teams without an external supervisor, user testing can serve the same purpose 
as a Rev 0 demo for the supervisor.}

\wss{This section might reference back to the SRS verification section.}

\section{System Tests}

This section describes the system-level tests for both functional and nonfunctional requirements. Functional requirement tests verify that the system implements the specified features correctly. Nonfunctional requirement tests validate quality attributes including performance, usability, reliability, security, scalability, and compatibility. Each test case specifies the test type, initial state, input conditions, expected output, and detailed procedures for execution.

\subsection{Tests for Functional Requirements}


\subsection{Tests for Nonfunctional Requirements}

The following subsections detail tests for system performance, AI matching quality, usability, reliability, data integrity, security, scalability, cross-browser compatibility, and performance benchmarking. These tests align with the nonfunctional requirements specified in the SRS document and provide measurable criteria for validating system quality attributes.

\subsubsection{System Performance Testing}

System performance testing validates that the platform meets specified performance requirements under expected workloads. These tests measure processing throughput for large fragment collections and system responsiveness under concurrent user load.

\paragraph{Collection Processing Performance}

These tests verify the system can process large collections of manuscript fragments within acceptable timeframes and handle concurrent research use as specified in the SRS.

\begin{enumerate}

\item \textbf{test-nfr-perf-1}

\textbf{Type:} Dynamic, Automated, Performance

\textbf{Initial State:} System deployed with empty database; sample dataset of 500 representative fragments from the British Library collection prepared for ingestion

\textbf{Input/Condition:} Execute bulk fragment processing pipeline with sample dataset; monitor processing time, resource utilization, and completion rate

\textbf{Output/Result:} Processing completes within 1.15 hours for 500 fragments; all fragments successfully processed with metadata extracted and indexed; system logs show no critical errors; extrapolated processing rate indicates full 21,000-fragment collection would process within $\leq$ 48 hours (per SRS NFR criteria)

\textbf{How test will be performed:} Automated script will initiate the bulk processing workflow on the 500-fragment sample. Performance monitoring tools (e.g., time command, system resource monitors) will track execution time, CPU usage, memory consumption, and I/O operations. Upon completion, validation queries will verify that all fragments are searchable with extracted metadata. Test passes if: (1) processing time $\leq$ 1.15 hours for 500 fragments (extrapolates to $\leq$ 48 hours for full 21,000-fragment collection, meeting SRS requirement), (2) success rate $\geq$ 99\%, and (3) no memory leaks or resource exhaustion detected. Processing rate will be calculated (fragments/hour) and documented for scaling projections.

\item \textbf{test-nfr-perf-2}

\textbf{Type:} Dynamic, Automated, Load Testing

\textbf{Initial State:} System fully operational with sample dataset (500-1000 fragments) loaded; load testing framework configured

\textbf{Input/Condition:} Simulate 15 concurrent users performing typical research workflows (searches, filtering, canvas operations) over a 20-minute period to verify system can support 15+ concurrent users (per SRS NFR criteria)

\textbf{Output/Result:} System maintains responsiveness throughout test period with 15 concurrent users; search queries return results within 3 seconds; canvas operations respond within 500ms; 95\% of operations complete successfully without errors; no server crashes or performance degradation

\textbf{How test will be performed:} Load testing tool (e.g., Locust or k6) will simulate 15 concurrent users executing scripted realistic research workflows. Test will ramp up from 1 to 15 users over 2 minutes, then maintain 15 concurrent users for 18 minutes. Test scenarios include: (1) search by fragment ID (10\% of operations), (2) metadata filter queries (40\%), (3) canvas workspace creation with 5-10 fragments (30\%), (4) session save/restore operations (20\%). Performance metrics (response times, error rates, throughput, server resource utilization) will be logged continuously. Test passes if: (1) mean search response time $\leq$ 2 seconds under load, (2) 95th percentile search time $\leq$ 3 seconds, (3) canvas operations complete within 500ms, (4) error rate $\leq$ 2\%, (5) system successfully handles 15 concurrent users without crashes (meeting SRS requirement), and (6) no unhandled exceptions or out-of-memory errors occur. Results will include response time graphs, throughput metrics, and resource utilization charts.

\end{enumerate}

\subsubsection{AI Matching Assistance Quality}

AI matching quality tests evaluate whether the system's machine learning-based fragment matching suggestions provide practical utility for scholarly research workflows.

\paragraph{Matching Suggestion Utility}

This test validates that AI-generated matching suggestions are sufficiently accurate and useful to assist researchers in identifying potential fragment connections.

\begin{enumerate}

\item \textbf{test-nfr-ai-1}

\textbf{Type:} Dynamic, Manual, Expert Evaluation

\textbf{Initial State:} System operational with AI matching model trained and deployed; test set of 20 fragment queries prepared with known ground truth (10 fragments with confirmed matches, 10 without matches in collection)

\textbf{Input/Condition:} For each test fragment, request AI matching suggestions; record top 5 suggestions and their confidence scores

\textbf{Output/Result:} For fragments with known matches: true match appears in top 5 suggestions in $\geq$ 60\% of cases; for fragments without matches: system correctly identifies low confidence or provides diverse alternatives; overall suggestion relevance rated as "potentially useful" by domain expert(s) in $\geq$ 50\% of cases

\textbf{How test will be performed:} Project supervisor and/or 1-2 domain experts (e.g., faculty member, graduate student in relevant field) will evaluate AI matching suggestions for the 30-fragment test set. For each fragment, evaluators will complete a standardized evaluation form.

\textbf{Evaluation Form (per fragment):}
\begin{itemize}
\item Fragment ID: [ID]
\item Known Ground Truth: [Has match in collection: Yes/No] [If yes, Fragment ID of match: \_\_\_]
\item Top 5 AI Suggestions: [List 5 suggestion IDs and confidence scores]
\item Ground Truth in Top 5?: [Yes/No]
\item Visual Feature Relevance: Do suggestions show similar edge patterns, damage, or script characteristics? [Yes/Somewhat/No]
\item Overall Utility Rating: Would these suggestions be useful for scholarly investigation? [Useful / Potentially Useful / Not Useful]
\item Comments: [Brief explanation of rating]
\end{itemize}

Results will be documented in a spreadsheet with one row per fragment. Test passes if: (1) known matches appear in top 5 for $\geq$ 60\% of test cases (6 out of 10 fragments with known matches), and (2) overall utility ratings are "useful" or "potentially useful" in $\geq$ 50\% of all 20 cases (10 or more fragments rated positively).

\end{enumerate}

\subsubsection{Usability Testing}

Usability testing evaluates whether the system interface enables scholars to effectively perform manuscript reconstruction research tasks. These tests combine task-based user testing and systematic interface evaluation.

\paragraph{Research Workflow Task Completion}

Task-based usability testing measures whether users can successfully complete core research workflows and assesses subjective ease of use.

\begin{enumerate}

\item \textbf{test-nfr-usability-1}

\textbf{Type:} Dynamic, Manual, User Testing

\textbf{Initial State:} System fully operational with sample dataset loaded; test participants (3-4 users: project supervisor and/or 2-3 classmates) recruited; standardized task scenarios prepared

\textbf{Input/Condition:} Each participant performs 5-6 representative research tasks: (1) search for specific fragment by ID, (2) apply metadata filters, (3) create canvas workspace, (4) arrange 3-5 fragments on canvas, (5) save session, (6) restore previous session

\textbf{Output/Result:} Task completion rate $\geq$ 80\% across all participants and tasks (per SRS NFR criteria); average task completion time within specified targets; no more than 1 critical usability issue preventing task completion; participants rate overall ease of use as "easy" or "very easy" for $\geq$ 70\% of tasks

\textbf{How test will be performed:} Participants will complete tasks in a moderated usability testing session (30-45 minutes per participant). For each task, observers will record: completion status (success/fail/partial), time-on-task, number of errors, assistance required, and participant comments. Task-level success criteria: Task 1-2 (search/filter): $\leq$ 2 minutes each; Task 3-4 (canvas): $\leq$ 3 minutes each; Task 5-6 (save/restore): $\leq$ 1 minute each.

Post-test questionnaire (5-point Likert scale for each task):
\begin{itemize}
\item Q1-Q6: "How easy was it to complete Task [1-6]?" (1=Very Difficult, 2=Difficult, 3=Neutral, 4=Easy, 5=Very Easy)
\item Q7: "What was the most confusing aspect of the system?" (open-ended)
\item Q8: "What feature did you find most helpful?" (open-ended)
\item Q9: "Would you recommend this system to colleagues for manuscript research?" (Yes/No/Maybe)
\end{itemize}

Test passes if: (1) overall task completion rate $\geq$ 80\% (aligns with SRS NFR requirement), (2) $\geq$ 70\% of individual task ratings (Q1-Q6) are 4 or 5 ("easy" or "very easy"), and (3) no more than 1 critical usability issue identified that completely blocks task completion.

\item \textbf{test-nfr-usability-2}

\textbf{Type:} Static, Manual, Interface Evaluation

\textbf{Initial State:} System interface fully implemented and accessible for evaluation; usability evaluation criteria checklist prepared

\textbf{Input/Condition:} 2 team members (who did not design the specific interface components being evaluated) independently evaluate the interface against core usability principles

\textbf{Output/Result:} Consolidated report identifying usability issues categorized by principle and severity (critical, major, minor); no more than 3 critical violations identified; prioritized list of recommended improvements documented

\textbf{How test will be performed:} Each team member evaluator will spend 1-2 hours systematically exploring the interface and documenting usability issues. Evaluators will use a standardized checklist covering core usability principles:

\textbf{Evaluation Checklist:}
\begin{enumerate}
\item \textbf{Clarity}: For each major interface element (search bar, filter panel, canvas controls), is it clear what actions are available and what they do? Rate: Pass/Fail with examples.
\item \textbf{Feedback}: When performing actions (search, drag fragment, save session), does the system provide clear visual or textual feedback? Rate: Pass/Fail with examples.
\item \textbf{Consistency}: Are similar operations (e.g., all "save" actions, all "delete" actions) performed in similar ways? Rate: Pass/Fail with examples.
\item \textbf{Error Prevention \& Recovery}: Can users prevent errors (confirmation dialogs) and recover from mistakes (undo, clear error messages)? Rate: Pass/Fail with examples.
\item \textbf{Efficiency}: Can users accomplish core tasks (search → filter → canvas → save) with minimal unnecessary steps? Count steps for each workflow. Rate: Pass/Fail.
\item \textbf{Learnability}: Can a new user understand how to perform basic search and canvas operations without documentation? Rate: Pass/Fail with observations.
\end{enumerate}

For each issue identified, document: (1) specific location/feature, (2) principle violated, (3) severity rating (critical/major/minor), (4) screenshot or description, (5) suggested fix. Severity definitions: critical (prevents task completion), major (causes significant frustration or confusion), minor (cosmetic or minor inconvenience). Team will meet to consolidate findings and create prioritized issue list. Test passes if: (1) no more than 3 critical violations identified, (2) evaluation report completed documenting all findings with specific examples, and (3) critical issues have documented remediation plans before final release.

\end{enumerate}

\subsubsection{System Availability and Reliability}

Reliability testing validates that the system maintains consistent availability for academic research and can recover from failures without data loss.

\paragraph{Uptime and Availability Testing}

These tests measure system uptime and verify recovery capabilities to ensure the platform reliably supports ongoing research activities.

\begin{enumerate}

\item \textbf{test-nfr-reliability-1}

\textbf{Type:} Dynamic, Automated, Reliability Testing

\textbf{Initial State:} System deployed to test/staging environment; GitHub Actions workflow configured for automated monitoring

\textbf{Input/Condition:} Continuous operation over 7-day period with automated health checks every 30 minutes via GitHub Actions scheduled workflow; simulated user activity via automated test script

\textbf{Output/Result:} System availability $\geq$ 95\% during the test period (per SRS NFR criteria); no more than 2 unplanned outages lasting $>$ 1 hour; system recovers automatically or can be restarted successfully after any failures

\textbf{How test will be performed:} GitHub Actions scheduled workflow (using cron syntax) will run every 30 minutes to perform health checks: (1) HTTP request to home page (expect 200 status), (2) execute test search query via API (verify JSON response), (3) verify database connectivity through health endpoint. Each workflow run logs results to a file committed to repository with timestamp and status. Additionally, a separate workflow runs hourly to simulate realistic user activity (perform searches, create canvas session, save/restore operations) and verify end-to-end functionality. All workflow runs are recorded in GitHub Actions history. For any failed health check: workflow creates GitHub issue automatically with failure details and timestamp. Test passes if: (1) uptime $\geq$ 95\% over 7 days (calculated as successful health checks / total health checks; allowing maximum 8.4 hours downtime), (2) no more than 2 unplanned outages lasting $>$ 1 hour, and (3) system recovers within 1 hour of failure detection (verified through subsequent health check passes). Results documented in availability report generated from workflow logs showing: total checks, successful checks, failed checks, outage periods, and uptime percentage.

\item \textbf{test-nfr-reliability-2}

\textbf{Type:} Dynamic, Manual, Disaster Recovery

\textbf{Initial State:} System operational with sample dataset loaded; backup procedures documented and tested; test environment available for recovery simulation

\textbf{Input/Condition:} Simulate critical failure scenario (e.g., accidental database deletion, container/server crash); initiate recovery procedures using documented backup strategy

\textbf{Output/Result:} System restored to functional state within 2 hours; sample dataset and user data successfully recovered with no loss; all core functionality (search, canvas, authentication) verified operational post-recovery

\textbf{How test will be performed:} Recovery test will be conducted in isolated test environment to avoid impacting production. Test scenario: (1) create backup of current system state (database, uploaded images, configuration), (2) simulate failure by stopping services and removing/corrupting critical data, (3) execute documented recovery procedures from backup, (4) measure time from "failure" to full restoration. Post-recovery verification checklist: database accessible and contains expected records, search returns correct results for sample queries, canvas workspace can be created and manipulated, user authentication works, uploaded images accessible. Test passes if: (1) recovery completed within 2 hours, (2) all verification checks pass, (3) no data loss detected (verified by record count and sample data spot-check), and (4) recovery procedure documentation is sufficient for team member unfamiliar with process to execute successfully.

\end{enumerate}

\subsubsection{Data Integrity and Security}

Data integrity and security testing ensures that fragment metadata and provenance information are preserved accurately and that the system protects against common security vulnerabilities.

\paragraph{Data Preservation and Attribution}

These tests verify that all fragment data maintains referential integrity and proper attribution throughout the system lifecycle.

\begin{enumerate}

\item \textbf{test-nfr-data-1}

\textbf{Type:} Functional, Automated, Data Validation
					
\textbf{Initial State:} British Library collection metadata and user-submitted images loaded into system
					
\textbf{Input/Condition:} Execute comprehensive data validation checks across entire collection
					
\textbf{Output/Result:} 100\% of fragments retain original British Library catalog information (shelfmark, collection, provenance); all user-submitted images maintain uploaded attribution; no corruption or loss of metadata fields
					
\textbf{How test will be performed:} Automated validation script will query all fragments in the database and verify metadata integrity.

\textbf{Required Metadata Fields:}
\begin{itemize}
\item \textbf{Fragment ID}: Unique identifier (non-null, unique constraint)
\item \textbf{Collection Source}: "British Library" or user-uploaded indicator (non-null)
\item \textbf{Shelfmark}: Original catalog reference (required for British Library fragments)
\item \textbf{Provenance}: Geographic/historical origin information (if available in source)
\item \textbf{Image URL/Path}: Location of fragment image (non-null, valid path)
\item \textbf{Upload Attribution}: Username or source for user-submitted fragments (non-null for user uploads)
\item \textbf{Timestamp}: Date added to system (non-null)
\end{itemize}

Validation checks: (1) all required fields present and non-null, (2) British Library fragments cross-referenced against source catalog CSV (shelfmark, collection matches), (3) user-uploaded fragments have valid attribution field, (4) referential integrity verified (foreign keys valid, no orphaned records). Script generates exception report listing any fragments with missing/inconsistent data. Manual spot-check of 100 randomly selected fragments verifies field accuracy. Test passes with 100\% of fragments having all required fields populated and $\geq$ 99\% matching source data exactly.

\item \textbf{test-nfr-data-2}

\textbf{Type:} Static, Manual, Code Review

\textbf{Initial State:} Data handling, authentication, and API code modules identified for review; security review checklist prepared based on OWASP guidelines

\textbf{Input/Condition:} Security-focused code review conducted by team members not primarily responsible for implementing the reviewed modules, supplemented by automated security scanning tools

\textbf{Output/Result:} Review checklist completed with findings documented; no critical security vulnerabilities identified; common security best practices verified (input validation, SQL injection prevention, authentication, authorization, API security)

\textbf{How test will be performed:} Two team members (preferably those who did not write the security-critical code) will conduct peer code review focusing on security aspects. Review checklist will cover: (1) input validation and sanitization for user-provided data, (2) SQL injection prevention (parameterized queries/ORM usage), (3) authentication mechanisms (password hashing, session management), (4) authorization controls (proper access checks before sensitive operations), (5) secure handling of API keys and secrets (not committed to repository), (6) protection against common web vulnerabilities (XSS, CSRF). Additionally, run automated security scanning tool appropriate for tech stack (e.g., npm audit for Node.js, Bandit for Python, pip-audit, or built-in IDE security warnings). All findings documented with severity ratings (critical/high/medium/low). Test passes if: (1) no critical or high-severity vulnerabilities identified in manual review, (2) checklist completed for all security-critical modules (authentication, data access, API endpoints), (3) automated scanning shows no critical issues, and (4) any medium/low findings have documented remediation plans or accepted risk justifications.

\end{enumerate}

\subsubsection{Scalability Testing}

Scalability testing analyzes how system performance changes as the fragment collection size increases, providing data to support future scaling decisions.

\paragraph{Fragment Collection Scalability}

This test measures system performance with varying dataset sizes to project scalability to larger fragment collections.

\begin{enumerate}

\item \textbf{test-nfr-scale-1}

\textbf{Type:} Dynamic, Manual, Scalability Analysis

\textbf{Initial State:} System operational with sample dataset (500 fragments); ability to add additional test data

\textbf{Input/Condition:} Test system performance with incrementally larger datasets: 500 fragments (baseline), 2,000 fragments, 5,000 fragments

\textbf{Output/Result:} Performance metrics documented for each dataset size showing how system scales; search response times remain acceptable ($\leq$ 5 seconds) at 5,000 fragments; documented analysis of expected performance at full 21,000-fragment scale based on observed trends

\textbf{How test will be performed:} Using existing sample data (500 fragments), create duplicate test data to reach 2,000 and 5,000 fragments (duplicates acceptable for testing purposes). At each dataset size, execute standardized queries: (1) fragment ID search (10 queries, measure average time), (2) single metadata filter (10 queries, measure average time), (3) multi-criteria filter with 2-3 filters (5 queries, measure average time). Record response times and database query execution times. Document results in simple table showing dataset size vs. average response time for each query type. Analyze trend (linear, logarithmic, exponential) and extrapolate to 21,000 fragments. Test passes if response times remain usable ($\leq$ 5 seconds) at 5,000 fragments and trend analysis suggests acceptable performance at full scale.

\end{enumerate}

\subsubsection{Cross-Browser and Device Compatibility}

Cross-browser compatibility testing ensures that the web-based interface functions correctly across major desktop browsers used by researchers.

\paragraph{Interface Compatibility Testing}

This test verifies that core functionality operates consistently across Chrome and at least one additional modern browser.

\begin{enumerate}

\item \textbf{test-nfr-compat-1}

\textbf{Type:} Manual, Dynamic, Compatibility Testing
					
\textbf{Initial State:} System deployed and accessible via URL
					
\textbf{Input/Condition:} Access system and perform core workflows on multiple browser and device combinations: Chrome, Firefox, Safari (desktop); Chrome, Safari (mobile); tablets
					
\textbf{Output/Result:} All core functionality (search, filter, canvas) operates correctly across tested platforms; visual layout renders appropriately; no critical browser-specific bugs identified
					
\textbf{How test will be performed:} Test team will execute standardized test suite on each browser/device combination.

\textbf{Test Suite (perform on each platform):}
\begin{enumerate}
\item \textbf{Search Test}: Enter fragment ID "BL-12345" in search bar, verify results display correctly
\item \textbf{Filter Test}: Apply metadata filter (e.g., "Script: Brahmi"), verify filtered results
\item \textbf{Canvas Creation}: Click "New Canvas", verify canvas workspace opens
\item \textbf{Drag-and-Drop}: Drag 2 fragment images onto canvas, verify they appear and can be repositioned
\item \textbf{Image Rendering}: Verify fragment images load at full resolution, zoom controls work
\item \textbf{Session Save}: Save canvas session, verify success message
\item \textbf{Session Restore}: Reload page, restore saved session, verify fragments reappear in correct positions
\item \textbf{Responsive Layout}: Resize browser window to mobile width (375px), verify layout adapts without breaking
\end{enumerate}

\textbf{Target Platforms:} Chrome (latest, primary development browser), Firefox (latest, secondary browser), Safari (latest macOS if team has access).

For each test-platform combination, record: Pass/Fail, any errors/warnings, visual issues, performance notes. Issues documented with severity (critical/major/minor) and browser-specific details. Test passes if all critical functionality works on Chrome and $\geq$ 1 additional browser. Critical issues on Chrome must be resolved; issues on secondary browsers documented for future work. Mobile testing is optional/out of scope for this project phase.

\end{enumerate}

\subsubsection{Performance Benchmarking}

Performance benchmarking establishes baseline metrics for system response times that can be used for regression testing in future development.

\paragraph{Response Time Measurement}

This test documents average response times for common operations to establish performance baselines.

\begin{enumerate}

\item \textbf{test-nfr-bench-1}

\textbf{Type:} Dynamic, Manual, Benchmark Testing

\textbf{Initial State:} System operational with sample dataset (500-1000 fragments)

\textbf{Input/Condition:} Execute standardized set of operations representing typical usage patterns: (1) fragment ID search, (2) single metadata filter, (3) multi-criteria filter (2-3 criteria), (4) canvas operations with 5-10 fragments

\textbf{Output/Result:} Performance report documenting average response times for each operation type; simple table showing mean response time for each operation; baseline metrics documented for future regression testing

\textbf{How test will be performed:} For each operation type, execute 10 test runs and record response times using browser developer tools or simple timing script. Calculate average (mean) response time for each operation type. Document results in simple table format: Operation Type | Average Response Time | Notes. Example acceptable targets: ID search $\leq$ 1 second, single filter $\leq$ 2 seconds, multi-filter $\leq$ 3 seconds, canvas operations $\leq$ 2 seconds. This benchmark establishes baseline performance metrics for future comparison rather than strict pass/fail criteria. Results provide data for identifying performance regressions in future releases.

\end{enumerate}

...

\subsection{Traceability Between Test Cases and Requirements}

\wss{Provide a table that shows which test cases are supporting which
  requirements.}

\section{Unit Test Description}

\wss{This section should not be filled in until after the MIS (detailed design
  document) has been completed.}

\wss{Reference your MIS (detailed design document) and explain your overall
philosophy for test case selection.}  

\wss{To save space and time, it may be an option to provide less detail in this section.  
For the unit tests you can potentially layout your testing strategy here.  That is, you 
can explain how tests will be selected for each module.  For instance, your test building 
approach could be test cases for each access program, including one test for normal behaviour 
and as many tests as needed for edge cases.  Rather than create the details of the input 
and output here, you could point to the unit testing code.  For this to work, you code 
needs to be well-documented, with meaningful names for all of the tests.}

\subsection{Unit Testing Scope}

\wss{What modules are outside of the scope.  If there are modules that are
  developed by someone else, then you would say here if you aren't planning on
  verifying them.  There may also be modules that are part of your software, but
  have a lower priority for verification than others.  If this is the case,
  explain your rationale for the ranking of module importance.}

\subsection{Tests for Functional Requirements}

\wss{Most of the verification will be through automated unit testing.  If
  appropriate specific modules can be verified by a non-testing based
  technique.  That can also be documented in this section.}

\subsubsection{Module 1}

\wss{Include a blurb here to explain why the subsections below cover the module.
  References to the MIS would be good.  You will want tests from a black box
  perspective and from a white box perspective.  Explain to the reader how the
  tests were selected.}

\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 
					
\item{test-id2\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input: 
					
Output: \wss{The expected result for the given inputs}

Test Case Derivation: \wss{Justify the expected value given in the Output field}

How test will be performed: 

\item{...\\}
    
\end{enumerate}

\subsubsection{Module 2}

...

\subsection{Tests for Nonfunctional Requirements}

\wss{If there is a module that needs to be independently assessed for
  performance, those test cases can go here.  In some projects, planning for
  nonfunctional tests of units will not be that relevant.}

\wss{These tests may involve collecting performance data from previously
  mentioned functional tests.}

\subsubsection{Module ?}
		
\begin{enumerate}

\item{test-id1\\}

Type: \wss{Functional, Dynamic, Manual, Automatic, Static etc. Most will
  be automatic}
					
Initial State: 
					
Input/Condition: 
					
Output/Result: 
					
How test will be performed: 
					
\item{test-id2\\}

Type: Functional, Dynamic, Manual, Static etc.
					
Initial State: 
					
Input: 
					
Output: 
					
How test will be performed: 

\end{enumerate}

\subsubsection{Module ?}

...

\subsection{Traceability Between Test Cases and Modules}

\wss{Provide evidence that all of the modules have been considered.}
				
\bibliographystyle{plainnat}

\bibliography{../../refs/References}

\newpage

\section{Appendix}

This is where you can place additional information.

\subsection{Symbolic Parameters}

The definition of the test cases will call for SYMBOLIC\_CONSTANTS.
Their values are defined in this section for easy maintenance.

\subsection{Usability Survey Questions?}

\wss{This is a section that would be appropriate for some projects.}

\newpage{}
\section*{Appendix --- Reflection}

\wss{This section is not required for CAS 741}

The information in this section will be used to evaluate the team members on the
graduate attribute of Lifelong Learning.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? 
  
  \hspace{2em} The fact that we had a solid base from our SRS to work from was one of this deliverable's strongest points. We were able to directly map the SRS's structured sections and clearly defined functional requirements into test areas, which greatly streamlined the planning process. Assigning ownership for test design and documentation was also simple because we had already finished the Development Plan and each of us knew our general responsibilities. This helped us stay organized. Throughout this phase, we worked effectively as a team, checked each other's sections for coherence, and developed trust that the verification plan related to our earlier work.

  \item What pain points did you experience during this deliverable, and how
    did you resolve them?

  \hspace{2em} Time management was our biggest challenge during this phase, as the deliverable overlapped with midterms and major assignments in other courses. We initially underestimated how much time would be required to properly structure the test cases and trace them back to the requirements. Since our platform integrates multiple components (front end, back end, and multiple AI/ML services), defining the boundaries between modules for testing was another significant challenge. This took longer than anticipated. In order to resolve this, we had brief group discussions to determine which system components would be verified at the unit level as opposed to the system level. These discussions also made sure that everyone understood the differences between validation and verification in the context of our platform. By the end, we were more assured of how we divided up the work and how we interpreted the testing scope.

  \item What knowledge and skills will the team collectively need to acquire to
  successfully complete the verification and validation of your project?
  Examples of possible knowledge and skills include dynamic testing knowledge,
  static testing knowledge, specific tool usage, Valgrind etc.  You should look to
  identify at least one item for each team member.\hspace{2em} To carry out the next stages of verification effectively, our team identified several areas for skill development. We need to strengthen our understanding of automated testing frameworks, particularly PyTest and coverage.py, to ensure consistent and maintainable test execution. We also plan to expand our familiarity with static and dynamic analysis tools like Ruff, MyPy, and Bandit for enforcing code quality and identifying potential vulnerabilities. Another important learning area will be validating the accuracy and robustness of our machine learning models — for example, evaluating OCR performance using accuracy metrics, confusion matrices, and ground truth comparison datasets. Collectively, these skills will help us achieve more objective, measurable verification results and reduce the likelihood of regression errors in later phases.

  \hspace{2em} To carry out the next stages of verification effectively, our team identified several areas for skill development. We need to strengthen our understanding of automated testing frameworks, particularly PyTest and coverage.py, to ensure consistent and maintainable test execution. We also plan to expand our familiarity with static and dynamic analysis tools like Ruff, MyPy, and Bandit for enforcing code quality and identifying potential vulnerabilities. Another important learning area will be validating the accuracy and robustness of our machine learning models, for example, evaluating OCR performance using accuracy metrics, confusion matrices, and ground truth comparison datasets. Collectively, these skills will help us achieve more objective, measurable verification results and reduce the likelihood of errors in later phases.

  \item For each of the knowledge areas and skills identified in the previous
  question, what are at least two approaches to acquiring the knowledge or
  mastering the skill?  Of the identified approaches, which will each team
  member pursue, and why did they make this choice?

  \hspace{2em} As a team, we plan to take a mixed approach that combines self-learning, collaborative practice, and direct application within the project. To establish a technical foundation, everyone in the group will adhere to the PyTest, coverage.py, and static analysis tools' official documentation and focused tutorials. Then, using GitHub Actions to run automated tests continuously and track coverage over time, we will further solidify this learning by implementing them early in the development process. We intend to incorporate brief knowledge-sharing periods into our weekly team meetings, where a team member can show the others a testing or verification method. In order to guide our validation setup for the AI components, we plan to review open-source evaluation scripts from Optical Character Recognition(OCR) and similarity-based matching research papers.  This collaborative, iterative approach ensures that every member not only understands the testing tools but also applies them meaningfully within their assigned subsystem.

\end{enumerate}

\end{document}